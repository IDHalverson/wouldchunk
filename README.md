# ğŸ§± wouldchunk

**wouldchunk** is an AI-powered content chunking microservice designed to intelligently process, segment, and prepare company documents, code, and data for downstream LLM applications like RAG, Q&A, and semantic search.

It takes in raw input (like a PDF, `.cob` file, Slack export, or Jira data), classifies it, routes it to the appropriate domain-specific chunker, uses an LLM (like Mistral via Ollama) to chunk the content meaningfully, and returns structured output with rich metadata.

## ğŸš€ Features

- ğŸ§  **LLM-Powered Semantic Chunking** â€” uses Mistral (locally via Ollama) to create meaningful chunks instead of naive slicing
- ğŸ“‚ **Multi-type Ingestion** â€” supports PDFs (coming soon: text files, code, Git logs, Slack exports, Jira issues, Jira comments, Confluence, Google Docs...)
- ğŸ” **Smart Routing** â€” auto-detects file type and content category (e.g. `"culture"`, `"code"`, `"git"`) and routes to the correct handler
- ğŸ—ï¸ **Single vs Multiple Handling** â€” dynamically chooses the right chunking pipeline based on number of files uploaded
- ğŸ·ï¸ **Metadata-Rich Output** â€” includes category, chunk index, timestamps, file source, versioning (more coming soon)
- ğŸ“œ **Output Logging** â€” every ingestion is saved as `.jsonl` for later reprocessing or training

## ğŸ”§ Requirements

- Python 3.10+
- [`ollama`](https://ollama.com) running locally with Mistral (`ollama run mistral`)
- `pdfplumber` for PDF ingestion
- `python-magic` for MIME sniffing

Install dependencies:

```bash
pip install -r requirements.txt
```

## ğŸ§ª Running the server

Start the FastAPI server:

```bash
uvicorn app.main:app --reload
```

## Docs

Access the API docs: (coming soon)

```bash
http://localhost:8000/docs
```

## ğŸ“¤ Testing

Testing with curl:

```bash
curl -X POST http://localhost:8000/auto-chunk \
  -F 'files=@/path/to/Your Document.pdf'
```

Youâ€™ll get back:
â€¢ A list of chunked content
â€¢ Metadata per chunk
â€¢ The fileâ€™s category and type
â€¢ The saved path to your .jsonl log file

Chunk Output Example:

```json
{
  "chunk_text": "Trust is earned. We do what we say.",
  "category": "culture",
  "chunk_index": 0,
  "source_file": "Our Company Culture.pdf",
  "version": "v1.0",
  "timestamp": "2025-04-08T22:30:00Z"
}
```

## ğŸ—ºï¸ Roadmap Ideas

- Add support for text files, code, Git logs, Slack exports, Jira issues, Jira comments, Confluence, Google Docs
- Unit tests
- Allow CLI-based batch ingestion
